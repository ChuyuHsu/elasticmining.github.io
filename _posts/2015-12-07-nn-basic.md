---
layout: post
title: Let's Play with Neural Network - Part 1
categories: neural network
abstract: "Basic Implementation in Matlab/Octave"
comments: true
tags: [neural network, algorithm, machine learning, matlab, octave]
image:
  feature:
  credit: 
  creditlink: 
date: 2015-12-07T00:00:00+00:00
author: ryanchao
---


### **Let's Play with Neural Network - Part 1**

#### **Basic Implementation in Matlab/Octave**
Neural network(NN) is an old but classic algorithm which could form non-linear and complex hypothesis by mimicking the brain behavior. With the progress in computer technology, today one can easily build an NN in personal computer to do something like time series prediction, multi-classification or image/speech recognition, playing with these algorithm is really cool and fun! So, before we could dive deeper in RNN[^1], CNN[^2] or even deep-learning, let’s recap some background knowledge and learn how to implement NN algorithm with some codes.

![nn_architecture](/img/posts/nn_arch.png)

The diagram above shows a shallow NN architecture which consists of (1) an input layer where we feed raw features $x$ in, (2) an output layer where we get the computational results $h_{\theta}(x)$ from the hypothesis, and (3) a hidden layer where the algorithm learns some complex features which helps NN to build stronger hypothesis. The function mapping between input/hidden/output layers is controlled by the matrices of weights $\Theta^{(i)}$. 

Like most of the other optimization problem, NN approaches the optimal hypothesis candidate by minimizing the cost function $J(\Theta)$ with some gradient decent techniques and a special algorithm called: back-propagation method, which is used to compute the partial derivative of the cost function respect to the weights, with that, the optimal weights could be determined by changing the values little by little in each gradient iteration, finally the cost function will converge and being trapped around a local minimum.

The cost function for multi-class classifier can be expressed as:


$$ \begin{align*} 
J(\Theta) = &-\frac{1}{m}\sum_{i = 1}^{m}\sum_{k = 1}^{K}\bigg[ y_{k}^{(i)}log(h_\theta(x^{(i)}))_{k} + (1 - y_{k}^{(i)})log(1 - h_{\theta}(x^{(i)})_k)\bigg]  \\
&+ \frac{\lambda}{2m}\sum_{l = 1}^{L}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2 
\end{align*}
$$


And the back-propagation process:


$$ \begin{align*} 
& \quad (1) \qquad \delta_{j}^{(L)} = h_{\theta}(x)_{j} - y_{j} \\
loop(l): & \begin{Bmatrix} 
 (2) \quad&\delta_{j}^{(l)} = (\Theta^{(l)})^{T}\delta^{(l+1)} .*(a^{(l)} .* (1 - a^{(l)}))      \\ 
  (3) \quad &\frac{\partial J(\Theta)}{\partial\Theta_{ij}^{(l)}} = a_{j}^{(l)}\delta_{i}^{(l+1)} + \frac{\lambda}{m}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_{l+1}}\Theta_{ji}^{(l)} \\ 
\end{Bmatrix}
 \end{align*} $$
 
Definitions of the notations:

> $J(\Theta)$: cost function, consists of the "error term" and the "regularization term".
> $m$: number of the samples in each "batch" we feed into NN.
> $K$: number of labels in multi-classification problem.
> $(x^{(i)}, y^{(i)})$: $(feature, \ label)$ pair of sample $i$.
> $L$: total number of $\Theta$, which is, the matrices used to do mapping from layer to layer.
> $\lambda$: regularization parameter.
> $\delta_{j}^{(L)}$: the "error" at node $j$ in the output layer.
> $\delta_{j}^{(l)}$: the algorithm loops the equation(2) and (3) to  compute the "error" backward in every hidden layer $l$ and get the partial derivative at every weight $\frac{\partial J}{\partial \Theta_{ij}}$.
> $a_{j}^{(l)}$: the activation value at node $j$ in hidden layer $l$.
> $.*$: symbol of element-wise multiplication.[^4] 
You can find more mathematical details from the reference[^3].

In short, to make the NN work, one should implement these two essential parts: (1) the codes to compute the cost function $J(\Theta)$ via forward propagation(i.e., the mapping series from input layer to output), and (2) the codes to measure the partial derivative of the cost function via back propagation. Then feed both into some optimization algorithm and the optimal weights will return as we expect.

The following codes implements the part (1)


```matlab
A = [ones(m, 1) X];           % raw features with m samples
Z_list{1} = X;
for i = 1 : Theta_num         % forward propagation process
    W = theta_list{i};        % matrix of weights between layers, 
                              % we use a cell array: “theta_list” to store all the matrices
                              % and extract them one by one to do mapping
    
    Z_list{i + 1} = A * W';   % intermediate variable of function mapping
    
    A = [ones(m, 1) sigmoid(Z_list{i + 1})];           %  we use the sigmoid(logistic) function as activation in NN
    J = J + sum(sum(W(:, 2 : end) .* W(:, 2 : end)));  % accumulates the weigths for regularization
end

O = sigmoid(Z_list{end});     % output layer
[p q] = size(O);
J = J * lambda / (2 * m);     % regularization part in cost function
J = J - sum(sum(Y .* log(O) ...
    + (ones(p, q) - Y) .* log(ones(p, q) - O))) / m;   % error part in cost function

return J;

```


And the following codes implements the part (2)


```matlab
del = O - Y;   % difference between output layer and target label; the output error
W = theta_list{theta_num};

% we use a cell array to store the partial derivative of cost function derivative(of cost function) respect to each matrix of weights 
grad_list{theta_num} = del' * [ones(m, 1) sigmoid(Z_list{theta_num})] / m ...           % error part in partial derivative
             + lambda * [zeros(size(grad_list{theta_num} , 1), 1) W(:, 2 : end)] / m;   % regularization part in partial derivative


for i = theta_num - 1 : -1 : 1   % back-propagation process

    del = del * W(:, 2 : end) .* sigmoid_gradient(Z_list{i + 1});                % backward propagation of error in hidden layers
    W = theta_list{i};
    if i > 1
        Z = sigmoid(Z_list{i});
    else
        Z = Z_list{i};
    end
    grad_list{i} = del' * [ones(m, 1) Z] / m ...                                 % error part in partial derivative
              + lambda * [zeros(size(grad_list{i} , 1), 1) W(:, 2 : end)] / m;   % regularization part in partial derivative
end

return grad_list;

```
*ps. cell array is a very convenient data structure, we could store different type/size of data in a cell container.[^5]* 

While the computation is perfectly vectorized, the algorithm we implemented can easily adapt to any size of input layer or any number of hidden layers(as long as the computing power is strong enough to withstand). For practical use, we combined the coding of cost function and its gradient computation in a single script(as a function) in order to feed them into some optimizer readily. If you're interested, here's the [source code](https://gist.github.com/ryanchao2012/5915befea3f3d426fcc1#file-fn_nncostfunction-m) of the complete implementation, we also run gradient checking by numerical approach to ensure its correctness.


Reference:

[^1]: https://en.wikipedia.org/wiki/Recurrent_neural_network

[^2]: https://en.wikipedia.org/wiki/Convolutional_neural_network

[^3]: Machine learning lectures made by professor Andrew Ng. https://www.coursera.org/learn/machine-learning

[^4]: http://www.mathworks.com/help/matlab/ref/times.html

[^5]: http://www.mathworks.com/help/matlab/cell-arrays.html





